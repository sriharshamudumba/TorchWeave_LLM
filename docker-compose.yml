version: '3.8'

name: torchweave_llm_server

volumes:
  artifacts: {}
  model_cache: {}

networks:
  torchweave_network:
    driver: bridge

services:
  # Model Manager Service - Handles dynamic model loading
  model-manager:
    build:
      context: ./model-manager
      dockerfile: Dockerfile
    env_file:
      - .env
    volumes:
      - artifacts:/artifacts
      - model_cache:/model_cache
    environment:
      - ARTIFACTS_DIR=/artifacts
      - MODEL_CACHE_DIR=/model_cache
      - HUGGINGFACE_CACHE_DIR=/model_cache/huggingface
      - SERVER_URL=http://server:8000
    networks:
      - torchweave_network
    command: ["python", "-m", "src.model_manager"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    ports:
      - "8001:8001"

  # Main inference server
  server:
    build:
      context: ./server
      dockerfile: Dockerfile
    env_file:
      - .env
    ports:
      - "8000:8000"
    volumes:
      - artifacts:/artifacts
      - model_cache:/model_cache
      - ./server/src:/app/src:ro
    environment:
      - MODEL_MANAGER_URL=http://model-manager:8001
      - ARTIFACTS_DIR=/artifacts
      - MODEL_CACHE_DIR=/model_cache
    command: ["uvicorn", "src.server:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]
    depends_on:
      - optimizer
      - model-manager
    networks:
      - torchweave_network
    # GPU configuration
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Model optimizer service
  optimizer:
    build:
      context: ./optimizer
      dockerfile: Dockerfile
    env_file:
      - .env
    volumes:
      - artifacts:/artifacts
      - model_cache:/model_cache
    environment:
      - ARTIFACTS_DIR=/artifacts
      - MODEL_CACHE_DIR=/model_cache
    networks:
      - torchweave_network
    command: ["python", "-m", "src.optimizer"]

  # Static HTML UI service
  ui:
    image: nginx:alpine
    ports:
      - "3000:80"
    volumes:
      - ./ui:/usr/share/nginx/html:ro
     # - ./nginx.conf:/etc/nginx/conf.d/default.conf:ro
    depends_on:
      - server
      - model-manager
    networks:
      - torchweave_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Redis for caching and session management (optional)
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - ./redis.conf:/usr/local/etc/redis/redis.conf
    command: redis-server /usr/local/etc/redis/redis.conf
    networks:
      - torchweave_network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
