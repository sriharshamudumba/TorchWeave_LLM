name: torchweave_llm_server

volumes:
  artifacts: {}

services:
  server:
    build:
      context: ./server
      dockerfile: Dockerfile
    env_file:
      - .env
    ports:
      - "8000:8000"
    volumes:
      - artifacts:/artifacts
      - ./server/src:/app/src:ro
    command: ["uvicorn", "src.server:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]
    # GPU access (falls back to CPU if unavailable)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: ["gpu"]
    gpus: all

  optimizer:
    build:
      context: ./optimizer
      dockerfile: Dockerfile
    env_file:
      - .env
    volumes:
      - artifacts:/artifacts
    command: ["python", "-m", "src.optimizer"]
